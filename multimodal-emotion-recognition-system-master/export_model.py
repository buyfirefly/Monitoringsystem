#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¼å‡ºè„šæœ¬
-------------------------------------------------
åŠŸèƒ½:
  1. å¯¼å‡ºè®­ç»ƒå¥½çš„å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«æ¨¡å‹
  2. æ”¯æŒå¤šç§æ ¼å¼: ONNX, TorchScript, æ ‡å‡†PyTorch
  3. æ¨¡å‹ä¼˜åŒ–å’Œé‡åŒ–
  4. å¯¼å‡ºé…ç½®å’Œå…ƒæ•°æ®

ä½¿ç”¨ç¤ºä¾‹:
  python export_model.py --model_path ./checkpoints/best_model.pth --format onnx
  python export_model.py --model_path ./checkpoints/best_model.pth --format torchscript --optimize
"""

import argparse
import torch
import torch.nn as nn
import onnx
import onnxruntime as ort
import json
from pathlib import Path
import numpy as np
from typing import Dict, Any, Tuple
import warnings

# å¯¼å…¥æ¨¡å‹
from multimodal_emotion.fusion_model import create_model


class ModelExporter:
    """æ¨¡å‹å¯¼å‡ºå™¨"""
    
    def __init__(self, model_path: str, output_dir: str = "./exported_models"):
        self.model_path = Path(model_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹å’Œé…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model, self.config = self._load_model()
        
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        print(f"æ¨¡å‹é…ç½®: {self.config}")
        
    def _load_model(self) -> Tuple[nn.Module, Dict[str, Any]]:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        
        # è·å–é…ç½®
        config = checkpoint.get('config', {
            'fusion_type': 'cross_attention',
            'visual_dim': 256,
            'audio_dim': 256,
            'hidden_dim': 512,
            'num_classes': 7,
            'dropout': 0.1
        })
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(
            fusion_type=config.get('fusion_type', 'cross_attention'),
            visual_dim=config.get('visual_dim', 256),
            audio_dim=config.get('audio_dim', 256),
            hidden_dim=config.get('hidden_dim', 512),
            num_classes=config.get('num_classes', 7),
            dropout=config.get('dropout', 0.1)
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        return model, config
    
    def export_pytorch(self, filename: str = "model.pth") -> str:
        """å¯¼å‡ºæ ‡å‡†PyTorchæ ¼å¼"""
        output_path = self.output_dir / filename
        
        # ä¿å­˜å®Œæ•´æ¨¡å‹ä¿¡æ¯
        export_data = {
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'model_class': 'MultimodalEmotionModel',
            'input_shapes': {
                'visual': [3, 224, 224],  # RGBå›¾åƒ
                'audio': [64, 500]        # æ¢…å°”é¢‘è°±å›¾
            },
            'output_shape': [self.config.get('num_classes', 7)],
            'emotion_labels': ['neutral', 'happiness', 'sadness', 'anger', 'fear', 'disgust', 'surprise']
        }
        
        torch.save(export_data, output_path)
        print(f"âœ… PyTorchæ¨¡å‹å·²å¯¼å‡º: {output_path}")
        return str(output_path)
    
    def export_torchscript(self, filename: str = "model.pt", optimize: bool = False) -> str:
        """å¯¼å‡ºTorchScriptæ ¼å¼"""
        output_path = self.output_dir / filename
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        batch_size = 1
        visual_input = torch.randn(batch_size, 3, 224, 224).to(self.device)
        audio_input = torch.randn(batch_size, 64, 500).to(self.device)
        
        try:
            # ä½¿ç”¨torch.jit.traceè¿›è¡Œè¿½è¸ª
            with torch.no_grad():
                traced_model = torch.jit.trace(self.model, (visual_input, audio_input))
            
            # å¯é€‰ä¼˜åŒ–
            if optimize:
                traced_model = torch.jit.optimize_for_inference(traced_model)
                print("ğŸš€ TorchScriptæ¨¡å‹å·²ä¼˜åŒ–")
            
            # ä¿å­˜æ¨¡å‹
            traced_model.save(str(output_path))
            
            # éªŒè¯å¯¼å‡ºçš„æ¨¡å‹
            loaded_model = torch.jit.load(str(output_path))
            with torch.no_grad():
                original_output = self.model(visual_input, audio_input)
                traced_output = loaded_model(visual_input, audio_input)
                diff = torch.abs(original_output - traced_output).max().item()
                print(f"ğŸ“Š æ¨¡å‹éªŒè¯: æœ€å¤§å·®å¼‚ = {diff:.6f}")
            
            print(f"âœ… TorchScriptæ¨¡å‹å·²å¯¼å‡º: {output_path}")
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
            return None
    
    def export_onnx(self, filename: str = "model.onnx", opset_version: int = 11) -> str:
        """å¯¼å‡ºONNXæ ¼å¼"""
        output_path = self.output_dir / filename
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        batch_size = 1
        visual_input = torch.randn(batch_size, 3, 224, 224).to(self.device)
        audio_input = torch.randn(batch_size, 64, 500).to(self.device)
        
        try:
            # å¯¼å‡ºONNX
            torch.onnx.export(
                self.model,
                (visual_input, audio_input),
                str(output_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['visual_input', 'audio_input'],
                output_names=['emotion_logits'],
                dynamic_axes={
                    'visual_input': {0: 'batch_size'},
                    'audio_input': {0: 'batch_size'},
                    'emotion_logits': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            onnx_model = onnx.load(str(output_path))
            onnx.checker.check_model(onnx_model)
            
            # æµ‹è¯•æ¨ç†
            ort_session = ort.InferenceSession(str(output_path))
            ort_inputs = {
                'visual_input': visual_input.cpu().numpy(),
                'audio_input': audio_input.cpu().numpy()
            }
            ort_outputs = ort_session.run(None, ort_inputs)
            
            # æ¯”è¾ƒè¾“å‡º
            with torch.no_grad():
                torch_output = self.model(visual_input, audio_input).cpu().numpy()
            diff = np.abs(torch_output - ort_outputs[0]).max()
            print(f"ğŸ“Š ONNXéªŒè¯: æœ€å¤§å·®å¼‚ = {diff:.6f}")
            
            print(f"âœ… ONNXæ¨¡å‹å·²å¯¼å‡º: {output_path}")
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            return None
    
    def export_quantized(self, filename: str = "model_quantized.pth") -> str:
        """å¯¼å‡ºé‡åŒ–æ¨¡å‹ï¼ˆå‡å°æ¨¡å‹å¤§å°ï¼‰"""
        output_path = self.output_dir / filename
        
        try:
            # åŠ¨æ€é‡åŒ–
            quantized_model = torch.quantization.quantize_dynamic(
                self.model.cpu(),
                {nn.Linear, nn.Conv2d},
                dtype=torch.qint8
            )
            
            # ä¿å­˜é‡åŒ–æ¨¡å‹
            export_data = {
                'model': quantized_model,
                'config': self.config,
                'quantized': True,
                'emotion_labels': ['neutral', 'happiness', 'sadness', 'anger', 'fear', 'disgust', 'surprise']
            }
            
            torch.save(export_data, output_path)
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            original_size = self.model_path.stat().st_size / (1024 * 1024)  # MB
            quantized_size = output_path.stat().st_size / (1024 * 1024)  # MB
            compression_ratio = original_size / quantized_size
            
            print(f"ğŸ“¦ æ¨¡å‹é‡åŒ–å®Œæˆ:")
            print(f"   åŸå§‹å¤§å°: {original_size:.2f} MB")
            print(f"   é‡åŒ–å¤§å°: {quantized_size:.2f} MB")
            print(f"   å‹ç¼©æ¯”: {compression_ratio:.2f}x")
            print(f"âœ… é‡åŒ–æ¨¡å‹å·²å¯¼å‡º: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
            return None
    
    def export_metadata(self, filename: str = "model_info.json") -> str:
        """å¯¼å‡ºæ¨¡å‹å…ƒæ•°æ®"""
        output_path = self.output_dir / filename
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        metadata = {
            'model_info': {
                'name': 'Multimodal Emotion Recognition Model',
                'version': '1.0.0',
                'framework': 'PyTorch',
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'model_size_mb': self.model_path.stat().st_size / (1024 * 1024)
            },
            'config': self.config,
            'input_specs': {
                'visual': {
                    'shape': [3, 224, 224],
                    'dtype': 'float32',
                    'description': 'RGB face image, normalized to [0,1]'
                },
                'audio': {
                    'shape': [64, 500],
                    'dtype': 'float32', 
                    'description': 'Mel-spectrogram features'
                }
            },
            'output_specs': {
                'emotion_logits': {
                    'shape': [7],
                    'dtype': 'float32',
                    'description': 'Emotion classification logits'
                }
            },
            'emotion_labels': {
                0: 'neutral',
                1: 'happiness', 
                2: 'sadness',
                3: 'anger',
                4: 'fear',
                5: 'disgust',
                6: 'surprise'
            },
            'preprocessing': {
                'visual': {
                    'resize': [224, 224],
                    'normalize': {
                        'mean': [0.485, 0.456, 0.406],
                        'std': [0.229, 0.224, 0.225]
                    }
                },
                'audio': {
                    'sample_rate': 16000,
                    'n_mels': 64,
                    'hop_length': 512,
                    'win_length': 1024
                }
            },
            'performance': {
                'dataset': 'MELD',
                'accuracy': 'See training logs',
                'inference_time': 'Depends on hardware'
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¨¡å‹å…ƒæ•°æ®å·²å¯¼å‡º: {output_path}")
        return str(output_path)


def main():
    parser = argparse.ArgumentParser(description='Export Multimodal Emotion Recognition Model')
    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model checkpoint')
    parser.add_argument('--output_dir', type=str, default='./exported_models',
                       help='Output directory for exported models')
    parser.add_argument('--format', type=str, choices=['pytorch', 'torchscript', 'onnx', 'quantized', 'all'],
                       default='all', help='Export format')
    parser.add_argument('--optimize', action='store_true',
                       help='Optimize exported model (for TorchScript)')
    parser.add_argument('--opset_version', type=int, default=11,
                       help='ONNX opset version')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = ModelExporter(args.model_path, args.output_dir)
    
    print(f"\nğŸš€ å¼€å§‹å¯¼å‡ºæ¨¡å‹...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    exported_files = []
    
    # æ ¹æ®æ ¼å¼å¯¼å‡º
    if args.format in ['pytorch', 'all']:
        file_path = exporter.export_pytorch()
        if file_path:
            exported_files.append(file_path)
    
    if args.format in ['torchscript', 'all']:
        file_path = exporter.export_torchscript(optimize=args.optimize)
        if file_path:
            exported_files.append(file_path)
    
    if args.format in ['onnx', 'all']:
        file_path = exporter.export_onnx(opset_version=args.opset_version)
        if file_path:
            exported_files.append(file_path)
    
    if args.format in ['quantized', 'all']:
        file_path = exporter.export_quantized()
        if file_path:
            exported_files.append(file_path)
    
    # æ€»æ˜¯å¯¼å‡ºå…ƒæ•°æ®
    metadata_path = exporter.export_metadata()
    if metadata_path:
        exported_files.append(metadata_path)
    
    print(f"\nâœ… å¯¼å‡ºå®Œæˆ! å…±å¯¼å‡º {len(exported_files)} ä¸ªæ–‡ä»¶:")
    for file_path in exported_files:
        print(f"   ğŸ“„ {file_path}")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print(f"   - PyTorch: ç”¨äºPythonæ¨ç†")
    print(f"   - TorchScript: ç”¨äºC++éƒ¨ç½²æˆ–ç§»åŠ¨ç«¯")
    print(f"   - ONNX: ç”¨äºè·¨å¹³å°éƒ¨ç½²")
    print(f"   - Quantized: ç”¨äºèµ„æºå—é™ç¯å¢ƒ")


if __name__ == '__main__':
    main()