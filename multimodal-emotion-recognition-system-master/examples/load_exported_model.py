#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¼å‡ºæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
-------------------------------------------------
æ¼”ç¤ºå¦‚ä½•åŠ è½½å’Œä½¿ç”¨ä¸åŒæ ¼å¼çš„å¯¼å‡ºæ¨¡å‹è¿›è¡Œæ¨ç†

ä½¿ç”¨ç¤ºä¾‹:
  python examples/load_exported_model.py --model_path ./exported_models/model.pth --format pytorch
  python examples/load_exported_model.py --model_path ./exported_models/model.onnx --format onnx
"""

import argparse
import torch
import numpy as np
from pathlib import Path
import json
import time
from typing import Tuple

try:
    import onnxruntime as ort
except ImportError:
    print("Warning: onnxruntime not installed. ONNX models cannot be loaded.")
    ort = None


class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨ - æ”¯æŒå¤šç§æ ¼å¼"""
    
    def __init__(self, model_path: str, format_type: str = 'auto'):
        self.model_path = Path(model_path)
        self.format_type = format_type if format_type != 'auto' else self._detect_format()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æƒ…ç»ªæ ‡ç­¾
        self.emotion_labels = [
            'neutral', 'happiness', 'sadness', 'anger', 
            'fear', 'disgust', 'surprise'
        ]
        
        print(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹æ ¼å¼: {self.format_type}")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        
    def _detect_format(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼"""
        suffix = self.model_path.suffix.lower()
        if suffix == '.onnx':
            return 'onnx'
        elif suffix == '.pt':
            return 'torchscript'
        elif suffix == '.pth':
            return 'pytorch'
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
    
    def _load_model(self):
        """æ ¹æ®æ ¼å¼åŠ è½½æ¨¡å‹"""
        if self.format_type == 'pytorch':
            return self._load_pytorch_model()
        elif self.format_type == 'torchscript':
            return self._load_torchscript_model()
        elif self.format_type == 'onnx':
            return self._load_onnx_model()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {self.format_type}")
    
    def _load_pytorch_model(self):
        """åŠ è½½PyTorchæ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½PyTorchæ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹æ•°æ®
        model_data = torch.load(self.model_path, map_location=self.device, weights_only=False)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¼å‡ºçš„æ ¼å¼
        if 'model_state_dict' in model_data:
            # è¿™é‡Œéœ€è¦é‡æ–°åˆ›å»ºæ¨¡å‹æ¶æ„
            # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæ‚¨éœ€è¦å¯¼å…¥å¹¶åˆ›å»ºç›¸åº”çš„æ¨¡å‹
            print("âš ï¸  æ³¨æ„: éœ€è¦æ¨¡å‹æ¶æ„ä»£ç æ¥å®Œå…¨åŠ è½½PyTorchæ¨¡å‹")
            print(f"ğŸ“‹ æ¨¡å‹é…ç½®: {model_data.get('config', {})}")
            return model_data
        else:
            # ç›´æ¥åŠ è½½çš„æ¨¡å‹
            return model_data
    
    def _load_torchscript_model(self):
        """åŠ è½½TorchScriptæ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½TorchScriptæ¨¡å‹...")
        model = torch.jit.load(self.model_path, map_location=self.device)
        model.eval()
        return model
    
    def _load_onnx_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        if ort is None:
            raise ImportError("è¯·å®‰è£…onnxruntime: pip install onnxruntime")
        
        print("ğŸ“¦ åŠ è½½ONNXæ¨¡å‹...")
        
        # è®¾ç½®æ‰§è¡Œæä¾›è€…
        providers = ['CPUExecutionProvider']
        if torch.cuda.is_available():
            providers.insert(0, 'CUDAExecutionProvider')
        
        session = ort.InferenceSession(str(self.model_path), providers=providers)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“Š è¾“å…¥èŠ‚ç‚¹: {[inp.name for inp in session.get_inputs()]}")
        print(f"ğŸ“Š è¾“å‡ºèŠ‚ç‚¹: {[out.name for out in session.get_outputs()]}")
        
        return session
    
    def predict(self, visual_input: np.ndarray, audio_input: np.ndarray) -> Tuple[np.ndarray, str]:
        """è¿›è¡Œé¢„æµ‹"""
        start_time = time.time()
        
        if self.format_type == 'pytorch':
            # PyTorchæ ¼å¼éœ€è¦æ¨¡å‹æ¶æ„ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹
            print("âš ï¸  PyTorchæ¨¡å‹éœ€è¦å®Œæ•´çš„æ¨¡å‹æ¶æ„ä»£ç ")
            # è¿”å›éšæœºç»“æœä½œä¸ºç¤ºä¾‹
            logits = np.random.randn(7)
        
        elif self.format_type == 'torchscript':
            # TorchScriptæ¨ç†
            visual_tensor = torch.from_numpy(visual_input).float().to(self.device)
            audio_tensor = torch.from_numpy(audio_input).float().to(self.device)
            
            with torch.no_grad():
                logits = self.model(visual_tensor, audio_tensor)
                logits = logits.cpu().numpy().squeeze()
        
        elif self.format_type == 'onnx':
            # ONNXæ¨ç†
            inputs = {
                'visual_input': visual_input.astype(np.float32),
                'audio_input': audio_input.astype(np.float32)
            }
            outputs = self.model.run(None, inputs)
            logits = outputs[0].squeeze()
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        probabilities = self._softmax(logits)
        predicted_class = np.argmax(probabilities)
        predicted_emotion = self.emotion_labels[predicted_class]
        confidence = probabilities[predicted_class]
        
        inference_time = time.time() - start_time
        
        print(f"âš¡ æ¨ç†æ—¶é—´: {inference_time*1000:.2f}ms")
        print(f"ğŸ¯ é¢„æµ‹æƒ…ç»ª: {predicted_emotion} (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        return probabilities, predicted_emotion
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """è®¡ç®—softmaxæ¦‚ç‡"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def benchmark(self, num_runs: int = 100):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸƒ å¼€å§‹æ€§èƒ½æµ‹è¯• ({num_runs} æ¬¡æ¨ç†)...")
        
        # åˆ›å»ºéšæœºè¾“å…¥
        visual_input = np.random.randn(1, 3, 224, 224)
        audio_input = np.random.randn(1, 64, 500)
        
        times = []
        
        for i in range(num_runs):
            start_time = time.time()
            self.predict(visual_input, audio_input)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # ç»Ÿè®¡ç»“æœ
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} Â± {std_time:.2f} ms")
        print(f"   æœ€å¿«æ¨ç†æ—¶é—´: {min_time:.2f} ms")
        print(f"   æœ€æ…¢æ¨ç†æ—¶é—´: {max_time:.2f} ms")
        print(f"   ååé‡: {1000/avg_time:.1f} FPS")


def create_dummy_inputs() -> Tuple[np.ndarray, np.ndarray]:
    """åˆ›å»ºç¤ºä¾‹è¾“å…¥æ•°æ®"""
    # æ¨¡æ‹Ÿé¢éƒ¨å›¾åƒ (batch_size=1, channels=3, height=224, width=224)
    visual_input = np.random.randn(1, 3, 224, 224)
    
    # æ¨¡æ‹ŸéŸ³é¢‘æ¢…å°”é¢‘è°±å›¾ (batch_size=1, n_mels=64, time_steps=500)
    audio_input = np.random.randn(1, 64, 500)
    
    return visual_input, audio_input


def main():
    parser = argparse.ArgumentParser(description='Load and test exported models')
    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to exported model')
    parser.add_argument('--format', type=str, choices=['pytorch', 'torchscript', 'onnx', 'auto'],
                       default='auto', help='Model format')
    parser.add_argument('--benchmark', action='store_true',
                       help='Run performance benchmark')
    parser.add_argument('--num_runs', type=int, default=100,
                       help='Number of benchmark runs')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not Path(args.model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        loader = ModelLoader(args.model_path, args.format)
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        visual_input, audio_input = create_dummy_inputs()
        
        print(f"\nğŸ¬ å¼€å§‹æ¨ç†æµ‹è¯•...")
        print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶: è§†è§‰={visual_input.shape}, éŸ³é¢‘={audio_input.shape}")
        
        # è¿›è¡Œé¢„æµ‹
        probabilities, predicted_emotion = loader.predict(visual_input, audio_input)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nğŸ“ˆ è¯¦ç»†é¢„æµ‹ç»“æœ:")
        for i, (emotion, prob) in enumerate(zip(loader.emotion_labels, probabilities)):
            print(f"   {emotion:>10}: {prob:.3f} {'ğŸ¯' if i == np.argmax(probabilities) else ''}")
        
        # æ€§èƒ½æµ‹è¯•
        if args.benchmark:
            loader.benchmark(args.num_runs)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()