"""
å®æ—¶æ¨ç†æ¥å£
æ”¯æŒä»æ‘„åƒå¤´å’Œéº¦å…‹é£å®æ—¶é‡‡é›†æ•°æ®è¿›è¡Œæƒ…ç»ªè¯†åˆ«
"""

import cv2
import torch
import torchaudio
import numpy as np
import pyaudio
import threading
import queue
import time
from pathlib import Path
import argparse
from collections import deque
import warnings
warnings.filterwarnings('ignore')

from fusion_model import create_model
from data_utils import EmotionDatasetMELD


class RealTimeEmotionRecognizer:
    """å®æ—¶æƒ…ç»ªè¯†åˆ«å™¨"""
    
    def __init__(
        self,
        model_path: str,
        config_path: str = None,
        device: str = 'cpu',
        audio_duration: float = 3.0,
        update_interval: float = 0.5,
        confidence_threshold: float = 0.5
    ):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡
            audio_duration: éŸ³é¢‘ç‰‡æ®µé•¿åº¦ï¼ˆç§’ï¼‰
            update_interval: æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.device = torch.device(device)
        self.audio_duration = audio_duration
        self.update_interval = update_interval
        self.confidence_threshold = confidence_threshold
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path, config_path)
        
        # éŸ³é¢‘å‚æ•°
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.audio_buffer = deque(maxlen=int(self.sample_rate * audio_duration))
        self.audio_queue = queue.Queue()
        
        # è§†é¢‘å‚æ•°
        self.frame_buffer = None
        self.frame_lock = threading.Lock()
        
        # æƒ…ç»ªæ ‡ç­¾
        self.emotion_labels = ['anger', 'disgust', 'sadness', 'joy', 
                              'neutral', 'surprise', 'fear']
        
        # æƒ…ç»ªè¡¨æƒ…ç¬¦å·ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        self.emotion_emojis = {
            'anger': 'ğŸ˜ ', 'disgust': 'ğŸ¤¢', 'sadness': 'ğŸ˜¢',
            'joy': 'ğŸ˜„', 'neutral': 'ğŸ˜', 'surprise': 'ğŸ˜®', 'fear': 'ğŸ˜¨'
        }
        
        # å›¾åƒå’ŒéŸ³é¢‘å˜æ¢
        self._setup_transforms()
        
        # è¿è¡Œæ ‡å¿—
        self.running = False
    
    def _load_model(self, model_path: str, config_path: str = None):
        """åŠ è½½æ¨¡å‹"""
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # è·å–é…ç½®
        if config_path:
            import json
            with open(config_path, 'r') as f:
                config = json.load(f)
        else:
            config = checkpoint.get('config', {})
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_model(
            num_emotions=config.get('num_emotions', 7),
            d_model=config.get('d_model', 256),
            fusion_type=config.get('fusion_type', 'cross_attention')
        )
        
        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"Model loaded from {model_path}")
    
    def _setup_transforms(self):
        """è®¾ç½®å›¾åƒå’ŒéŸ³é¢‘å˜æ¢"""
        # å›¾åƒå˜æ¢
        from torchvision import transforms
        self.img_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
        
        # éŸ³é¢‘å˜æ¢ï¼ˆæ¢…å°”é¢‘è°±ï¼‰
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_mels=64,
            n_fft=512,
            hop_length=160,
            win_length=400
        )
        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """PyAudioå›è°ƒå‡½æ•°"""
        # å°†éŸ³é¢‘æ•°æ®æ”¾å…¥é˜Ÿåˆ—
        self.audio_queue.put(in_data)
        return (in_data, pyaudio.paContinue)
    
    def audio_thread(self):
        """éŸ³é¢‘é‡‡é›†çº¿ç¨‹"""
        p = pyaudio.PyAudio()
        
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self.audio_callback
        )
        
        stream.start_stream()
        
        while self.running:
            try:
                # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®
                audio_data = self.audio_queue.get(timeout=0.1)
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                self.audio_buffer.extend(audio_array)
            except queue.Empty:
                continue
        
        stream.stop_stream()
        stream.close()
        p.terminate()
    
    def video_thread(self):
        """è§†é¢‘é‡‡é›†çº¿ç¨‹"""
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        while self.running:
            ret, frame = cap.read()
            if ret:
                with self.frame_lock:
                    self.frame_buffer = frame.copy()
            time.sleep(0.03)  # ~30 FPS
        
        cap.release()
    
    def process_inputs(self):
        """å¤„ç†è¾“å…¥æ•°æ®å¹¶è¿›è¡Œæ¨ç†"""
        # è·å–å½“å‰å¸§
        with self.frame_lock:
            if self.frame_buffer is None:
                return None
            frame = self.frame_buffer.copy()
        
        # è·å–éŸ³é¢‘æ•°æ®
        if len(self.audio_buffer) < self.sample_rate * self.audio_duration:
            return None
        
        audio_data = np.array(list(self.audio_buffer))
        
        # é¢„å¤„ç†å›¾åƒ
        # æ£€æµ‹äººè„¸ï¼ˆå¯é€‰ï¼‰
        face_frame = self.detect_and_crop_face(frame)
        if face_frame is None:
            face_frame = frame  # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨æ•´ä¸ªå¸§
        
        # è½¬æ¢å›¾åƒ
        img_tensor = self.img_transform(face_frame).unsqueeze(0).to(self.device)
        
        # é¢„å¤„ç†éŸ³é¢‘
        # å½’ä¸€åŒ–éŸ³é¢‘
        audio_tensor = torch.from_numpy(audio_data.astype(np.float32) / 32768.0)
        audio_tensor = audio_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # è®¡ç®—æ¢…å°”é¢‘è°±
        mel_spec = self.mel_transform(audio_tensor)
        mel_spec = self.amplitude_to_db(mel_spec)
        mel_spec = mel_spec.squeeze(0).to(self.device)  # (n_mels, time)
        mel_spec = mel_spec.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(img_tensor, mel_spec)
            probs = torch.softmax(outputs, dim=1)
            confidence, pred_idx = probs.max(1)
        
        # è·å–ç»“æœ
        emotion = self.emotion_labels[pred_idx.item()]
        confidence_score = confidence.item()
        
        # åªæœ‰å½“ç½®ä¿¡åº¦è¶…è¿‡é˜ˆå€¼æ—¶æ‰è¿”å›ç»“æœ
        if confidence_score >= self.confidence_threshold:
            return {
                'emotion': emotion,
                'confidence': confidence_score,
                'probabilities': probs.squeeze().cpu().numpy(),
                'frame': frame
            }
        else:
            return None
    
    def detect_and_crop_face(self, frame):
        """æ£€æµ‹å¹¶è£å‰ªäººè„¸"""
        # ä½¿ç”¨OpenCVçš„äººè„¸æ£€æµ‹å™¨
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        
        if len(faces) > 0:
            # é€‰æ‹©æœ€å¤§çš„äººè„¸
            x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
            # æ‰©å±•è¾¹ç•Œæ¡†
            padding = int(0.2 * w)
            x = max(0, x - padding)
            y = max(0, y - padding)
            w = min(frame.shape[1] - x, w + 2 * padding)
            h = min(frame.shape[0] - y, h + 2 * padding)
            
            face_frame = frame[y:y+h, x:x+w]
            # è½¬æ¢ä¸ºRGB
            face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)
            return face_frame
        
        return None
    
    def draw_results(self, frame, result):
        """åœ¨å¸§ä¸Šç»˜åˆ¶ç»“æœ"""
        if result is None:
            return frame
        
        emotion = result['emotion']
        confidence = result['confidence']
        probs = result['probabilities']
        
        # ç»˜åˆ¶ä¸»è¦æƒ…ç»ª
        emoji = self.emotion_emojis.get(emotion, '')
        text = f"{emoji} {emotion.capitalize()}: {confidence:.2%}"
        cv2.putText(frame, text, (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)
        
        # ç»˜åˆ¶æ‰€æœ‰æƒ…ç»ªçš„æ¦‚ç‡æ¡
        bar_width = 200
        bar_height = 20
        start_y = 80
        
        for i, (label, prob) in enumerate(zip(self.emotion_labels, probs)):
            y = start_y + i * 30
            # èƒŒæ™¯æ¡
            cv2.rectangle(frame, (20, y), (20 + bar_width, y + bar_height),
                         (200, 200, 200), -1)
            # æ¦‚ç‡æ¡
            filled_width = int(bar_width * prob)
            color = (0, 255, 0) if label == emotion else (100, 100, 100)
            cv2.rectangle(frame, (20, y), (20 + filled_width, y + bar_height),
                         color, -1)
            # æ ‡ç­¾
            cv2.putText(frame, f"{label}: {prob:.2%}", 
                       (25 + bar_width, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        
        return frame
    
    def run(self):
        """è¿è¡Œå®æ—¶æƒ…ç»ªè¯†åˆ«"""
        self.running = True
        
        # å¯åŠ¨éŸ³é¢‘å’Œè§†é¢‘çº¿ç¨‹
        audio_thread = threading.Thread(target=self.audio_thread)
        video_thread = threading.Thread(target=self.video_thread)
        
        audio_thread.start()
        video_thread.start()
        
        print("Starting real-time emotion recognition...")
        print("Press 'q' to quit")
        
        last_update = time.time()
        current_result = None
        
        try:
            while self.running:
                # å®šæœŸæ›´æ–°é¢„æµ‹
                if time.time() - last_update >= self.update_interval:
                    result = self.process_inputs()
                    if result is not None:
                        current_result = result
                    last_update = time.time()
                
                # æ˜¾ç¤ºç»“æœ
                with self.frame_lock:
                    if self.frame_buffer is not None:
                        display_frame = self.frame_buffer.copy()
                        display_frame = self.draw_results(display_frame, current_result)
                        cv2.imshow('Multimodal Emotion Recognition', display_frame)
                
                # æ£€æŸ¥é€€å‡º
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.running = False
                    
        except KeyboardInterrupt:
            print("\nStopping...")
        finally:
            self.running = False
            audio_thread.join()
            video_thread.join()
            cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Real-time Multimodal Emotion Recognition')
    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model checkpoint')
    parser.add_argument('--config_path', type=str,
                       help='Path to model config (optional)')
    parser.add_argument('--device', type=str, default='cpu',
                       choices=['cpu', 'cuda', 'mps'])
    parser.add_argument('--audio_duration', type=float, default=3.0,
                       help='Audio segment duration in seconds')
    parser.add_argument('--update_interval', type=float, default=0.5,
                       help='Prediction update interval in seconds')
    parser.add_argument('--confidence_threshold', type=float, default=0.5,
                       help='Minimum confidence threshold for predictions')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®æ—¶è¯†åˆ«å™¨
    recognizer = RealTimeEmotionRecognizer(
        model_path=args.model_path,
        config_path=args.config_path,
        device=args.device,
        audio_duration=args.audio_duration,
        update_interval=args.update_interval,
        confidence_threshold=args.confidence_threshold
    )
    
    # è¿è¡Œ
    recognizer.run()


if __name__ == '__main__':
    main()
