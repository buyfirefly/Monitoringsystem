#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶æ‘„åƒå¤´æƒ…ç»ªç›‘æµ‹
-------------------------------------------------
åŠŸèƒ½:
  1. å®æ—¶æ•è·æ‘„åƒå¤´è§†é¢‘æµ
  2. æ£€æµ‹äººè„¸å¹¶æå–é¢éƒ¨è¡¨æƒ…
  3. å®æ—¶å½•åˆ¶éŸ³é¢‘å¹¶æå–è¯­éŸ³ç‰¹å¾
  4. ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œæƒ…ç»ªè¯†åˆ«
  5. å®æ—¶æ˜¾ç¤ºæƒ…ç»ªè¯†åˆ«ç»“æœ
  6. æ”¯æŒç»“æœä¿å­˜å’Œç»Ÿè®¡åˆ†æ

ä½¿ç”¨ç¤ºä¾‹:
  python realtime_camera_emotion.py --model_path ./checkpoints/best_model.pth
  python realtime_camera_emotion.py --model_path ./exported_models/model.onnx --save_results
"""

import cv2
import torch
import numpy as np
import pyaudio
import threading
import queue
import time
import argparse
from pathlib import Path
import json
from datetime import datetime
from collections import deque, Counter
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import librosa
from typing import Optional, Tuple, Dict, List

# å¯¼å…¥æ¨¡å‹ç›¸å…³æ¨¡å—
try:
    from multimodal_emotion.fusion_model import create_model
except ImportError:
    print("Warning: æ— æ³•å¯¼å…¥fusion_modelï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œ")

try:
    import onnxruntime as ort
except ImportError:
    print("Warning: onnxruntimeæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ONNXæ¨¡å‹")
    ort = None


class AudioCapture:
    """éŸ³é¢‘æ•è·ç±»"""
    
    def __init__(self, sample_rate=16000, chunk_size=1024, channels=1):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.channels = channels
        self.audio_queue = queue.Queue()
        self.is_recording = False
        
        # åˆå§‹åŒ–PyAudio
        self.audio = pyaudio.PyAudio()
        self.stream = None
        
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        self.is_recording = True
        self.stream = self.audio.open(
            format=pyaudio.paFloat32,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self._audio_callback
        )
        self.stream.start_stream()
        print("ğŸ¤ éŸ³é¢‘å½•åˆ¶å·²å¼€å§‹")
        
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        self.is_recording = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.audio.terminate()
        print("ğŸ¤ éŸ³é¢‘å½•åˆ¶å·²åœæ­¢")
        
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if self.is_recording:
            audio_data = np.frombuffer(in_data, dtype=np.float32)
            self.audio_queue.put(audio_data)
        return (None, pyaudio.paContinue)
        
    def get_audio_features(self, duration=3.0):
        """è·å–éŸ³é¢‘ç‰¹å¾"""
        # æ”¶é›†æŒ‡å®šæ—¶é•¿çš„éŸ³é¢‘æ•°æ®
        target_samples = int(self.sample_rate * duration)
        audio_buffer = []
        
        while len(audio_buffer) < target_samples:
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                audio_buffer.extend(chunk)
            except queue.Empty:
                break
                
        if len(audio_buffer) < target_samples:
            # å¦‚æœéŸ³é¢‘ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
            audio_buffer.extend([0.0] * (target_samples - len(audio_buffer)))
        else:
            # å¦‚æœéŸ³é¢‘è¿‡å¤šï¼Œæˆªå–
            audio_buffer = audio_buffer[:target_samples]
            
        audio_array = np.array(audio_buffer, dtype=np.float32)
        
        # æå–æ¢…å°”é¢‘è°±å›¾ç‰¹å¾
        mel_spec = librosa.feature.melspectrogram(
            y=audio_array,
            sr=self.sample_rate,
            n_mels=64,
            hop_length=512,
            win_length=1024
        )
        
        # è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # è°ƒæ•´åˆ°å›ºå®šå¤§å° (64, 500)
        if log_mel_spec.shape[1] < 500:
            # å¦‚æœæ—¶é—´ç»´åº¦ä¸å¤Ÿï¼Œé‡å¤å¡«å……
            repeat_times = 500 // log_mel_spec.shape[1] + 1
            log_mel_spec = np.tile(log_mel_spec, (1, repeat_times))
        
        log_mel_spec = log_mel_spec[:, :500]  # æˆªå–åˆ°500å¸§
        
        return log_mel_spec


class EmotionDetector:
    """æƒ…ç»ªæ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, model_format: str = 'auto'):
        self.model_path = Path(model_path)
        self.model_format = model_format if model_format != 'auto' else self._detect_format()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æƒ…ç»ªæ ‡ç­¾
        self.emotion_labels = [
            'neutral', 'happiness', 'sadness', 'anger',
            'fear', 'disgust', 'surprise'
        ]
        
        # æƒ…ç»ªé¢œè‰²æ˜ å°„ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.emotion_colors = {
            'neutral': (128, 128, 128),     # ç°è‰²
            'happiness': (0, 255, 0),       # ç»¿è‰²
            'sadness': (255, 0, 0),         # è“è‰²
            'anger': (0, 0, 255),           # çº¢è‰²
            'fear': (128, 0, 128),          # ç´«è‰²
            'disgust': (0, 128, 0),         # æ·±ç»¿è‰²
            'surprise': (255, 255, 0)       # é»„è‰²
        }
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_format} æ ¼å¼")
        
        # äººè„¸æ£€æµ‹å™¨
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
    def _detect_format(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼"""
        suffix = self.model_path.suffix.lower()
        if suffix == '.onnx':
            return 'onnx'
        elif suffix == '.pt':
            return 'torchscript'
        elif suffix == '.pth':
            return 'pytorch'
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
            
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model_format == 'onnx':
            return self._load_onnx_model()
        elif self.model_format == 'torchscript':
            return self._load_torchscript_model()
        elif self.model_format == 'pytorch':
            return self._load_pytorch_model()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {self.model_format}")
            
    def _load_onnx_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        if ort is None:
            raise ImportError("è¯·å®‰è£…onnxruntime: pip install onnxruntime")
            
        providers = ['CPUExecutionProvider']
        if torch.cuda.is_available():
            providers.insert(0, 'CUDAExecutionProvider')
            
        session = ort.InferenceSession(str(self.model_path), providers=providers)
        return session
        
    def _load_torchscript_model(self):
        """åŠ è½½TorchScriptæ¨¡å‹"""
        model = torch.jit.load(self.model_path, map_location=self.device)
        model.eval()
        return model
        
    def _load_pytorch_model(self):
        """åŠ è½½PyTorchæ¨¡å‹"""
        checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        state_dict = checkpoint['model_state_dict']
        
        # å¦‚æœæ¨¡å‹åŒ…å«classifierå±‚ï¼Œè¯´æ˜æ˜¯æ—§ç‰ˆæœ¬çš„æ¨¡å‹ç»“æ„
        if any('classifier' in key for key in state_dict.keys()):
            print("æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ¨¡å‹ç»“æ„ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼åŠ è½½...")
            # ç›´æ¥ä»state_dicté‡å»ºæ¨¡å‹
            from multimodal_emotion.fusion_model import MultiModalEmotionModel
            
            # æ ¹æ®state_dictæ¨æ–­æ¨¡å‹å‚æ•°
            visual_dim = 256  # ä»classifierå±‚æ¨æ–­
            audio_dim = 256
            hidden_dim = 512
            num_classes = 7
            
            model = MultiModalEmotionModel(
                visual_dim=visual_dim,
                audio_dim=audio_dim, 
                hidden_dim=hidden_dim,
                num_classes=num_classes
            )
            
            # åŠ è½½æƒé‡ï¼Œå¿½ç•¥ä¸åŒ¹é…çš„å±‚
            model.load_state_dict(state_dict, strict=False)
        else:
            # æ–°ç‰ˆæœ¬æ¨¡å‹ç»“æ„
            config = checkpoint.get('config', {})
            model = create_model(
                num_emotions=config.get('num_classes', 7),
                d_model=config.get('d_model', 256),
                fusion_type=config.get('fusion_type', 'cross_attention'),
                pretrained=config.get('pretrained', True)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
        
        model.to(self.device)
        model.eval()
        return model
        
    def detect_faces(self, frame):
        """æ£€æµ‹äººè„¸"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
        )
        return faces
        
    def preprocess_face(self, face_img):
        """é¢„å¤„ç†äººè„¸å›¾åƒ"""
        # è°ƒæ•´å¤§å°åˆ°224x224
        face_resized = cv2.resize(face_img, (224, 224))
        
        # è½¬æ¢ä¸ºRGB
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        
        # å½’ä¸€åŒ–
        face_normalized = face_rgb.astype(np.float32) / 255.0
        
        # ImageNetæ ‡å‡†åŒ–
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        face_normalized = (face_normalized - mean) / std
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶æ·»åŠ batchç»´åº¦
        face_tensor = np.transpose(face_normalized, (2, 0, 1))
        face_batch = np.expand_dims(face_tensor, axis=0)
        
        return face_batch
        
    def predict_emotion(self, visual_input, audio_input):
        """é¢„æµ‹æƒ…ç»ª"""
        if self.model_format == 'onnx':
            # ONNXæ¨ç†
            inputs = {
                'visual_input': visual_input.astype(np.float32),
                'audio_input': audio_input.astype(np.float32)
            }
            outputs = self.model.run(None, inputs)
            logits = outputs[0].squeeze()
        else:
            # PyTorch/TorchScriptæ¨ç†
            visual_tensor = torch.from_numpy(visual_input).float().to(self.device)
            audio_tensor = torch.from_numpy(audio_input).float().to(self.device)
            
            with torch.no_grad():
                logits = self.model(visual_tensor, audio_tensor)
                logits = logits.cpu().numpy().squeeze()
                
        # è®¡ç®—æ¦‚ç‡
        probabilities = self._softmax(logits)
        predicted_class = np.argmax(probabilities)
        predicted_emotion = self.emotion_labels[predicted_class]
        confidence = probabilities[predicted_class]
        
        return predicted_emotion, confidence, probabilities
        
    def _softmax(self, x):
        """è®¡ç®—softmaxæ¦‚ç‡"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)


class RealtimeEmotionMonitor:
    """å®æ—¶æƒ…ç»ªç›‘æµ‹å™¨"""
    
    def __init__(self, model_path: str, save_results: bool = False, 
                 show_stats: bool = True, camera_id: int = 0):
        self.detector = EmotionDetector(model_path)
        self.audio_capture = AudioCapture()
        self.save_results = save_results
        self.show_stats = show_stats
        self.camera_id = camera_id
        
        # ç»“æœå­˜å‚¨
        self.emotion_history = deque(maxlen=100)  # ä¿å­˜æœ€è¿‘100æ¬¡æ£€æµ‹ç»“æœ
        self.results_log = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.frame_count = 0
        self.start_time = time.time()
        
        # å¯è§†åŒ–è®¾ç½®
        if self.show_stats:
            self.setup_visualization()
            
    def setup_visualization(self):
        """è®¾ç½®å¯è§†åŒ–ç•Œé¢"""
        plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æƒ…ç»ªåˆ†å¸ƒé¥¼å›¾
        self.ax1.set_title('æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡')
        
        # æƒ…ç»ªæ—¶é—´åºåˆ—
        self.ax2.set_title('æƒ…ç»ªå˜åŒ–è¶‹åŠ¿')
        self.ax2.set_xlabel('æ—¶é—´')
        self.ax2.set_ylabel('ç½®ä¿¡åº¦')
        
    def update_visualization(self):
        """æ›´æ–°å¯è§†åŒ–ç•Œé¢"""
        if not self.show_stats or len(self.emotion_history) < 5:
            return
            
        # æ¸…é™¤ä¹‹å‰çš„å›¾
        self.ax1.clear()
        self.ax2.clear()
        
        # æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡
        emotions = [result['emotion'] for result in self.emotion_history]
        emotion_counts = Counter(emotions)
        
        if emotion_counts:
            labels = list(emotion_counts.keys())
            sizes = list(emotion_counts.values())
            colors = [self.detector.emotion_colors.get(emotion, (128, 128, 128)) for emotion in labels]
            colors = [(r/255, g/255, b/255) for r, g, b in colors]  # å½’ä¸€åŒ–é¢œè‰²
            
            self.ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
            self.ax1.set_title('æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡')
        
        # æƒ…ç»ªæ—¶é—´åºåˆ—
        if len(self.emotion_history) > 1:
            times = [result['timestamp'] for result in self.emotion_history]
            confidences = [result['confidence'] for result in self.emotion_history]
            emotions = [result['emotion'] for result in self.emotion_history]
            
            # ä¸ºä¸åŒæƒ…ç»ªä½¿ç”¨ä¸åŒé¢œè‰²
            unique_emotions = list(set(emotions))
            for emotion in unique_emotions:
                emotion_times = [t for t, e in zip(times, emotions) if e == emotion]
                emotion_confidences = [c for c, e in zip(confidences, emotions) if e == emotion]
                color = self.detector.emotion_colors.get(emotion, (128, 128, 128))
                color = (color[0]/255, color[1]/255, color[2]/255)
                
                self.ax2.scatter(emotion_times, emotion_confidences, 
                               c=[color], label=emotion, alpha=0.7)
            
            self.ax2.set_title('æƒ…ç»ªå˜åŒ–è¶‹åŠ¿')
            self.ax2.set_xlabel('æ—¶é—´ (ç§’)')
            self.ax2.set_ylabel('ç½®ä¿¡åº¦')
            self.ax2.legend()
            self.ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.pause(0.01)  # çŸ­æš‚æš‚åœä»¥æ›´æ–°å›¾å½¢
        
    def run(self):
        """è¿è¡Œå®æ—¶ç›‘æµ‹"""
        print("ğŸš€ å¯åŠ¨å®æ—¶æƒ…ç»ªç›‘æµ‹...")
        print("æŒ‰ 'q' é”®é€€å‡ºï¼ŒæŒ‰ 's' é”®ä¿å­˜å½“å‰ç»“æœ")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        cap = cv2.VideoCapture(self.camera_id)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
            
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ
        
        # å¼€å§‹éŸ³é¢‘å½•åˆ¶
        self.audio_capture.start_recording()
        
        try:
            last_prediction_time = 0
            prediction_interval = 0.2  # æ¯200msè¿›è¡Œä¸€æ¬¡æƒ…ç»ªé¢„æµ‹
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                    break
                    
                self.frame_count += 1
                current_time = time.time() - self.start_time
                
                # æ£€æµ‹äººè„¸
                faces = self.detector.detect_faces(frame)
                
                if len(faces) > 0:
                    # ä½¿ç”¨æœ€å¤§çš„äººè„¸
                    largest_face = max(faces, key=lambda f: f[2] * f[3])
                    x, y, w, h = largest_face
                    
                    # åªåœ¨æŒ‡å®šé—´éš”å†…è¿›è¡Œæƒ…ç»ªé¢„æµ‹
                    if current_time - last_prediction_time >= prediction_interval:
                        # æå–äººè„¸åŒºåŸŸ
                        face_img = frame[y:y+h, x:x+w]
                        
                        # é¢„å¤„ç†äººè„¸å›¾åƒ
                        visual_input = self.detector.preprocess_face(face_img)
                        
                        # è·å–éŸ³é¢‘ç‰¹å¾
                        audio_input = self.audio_capture.get_audio_features()
                        audio_input = np.expand_dims(audio_input, axis=0)  # æ·»åŠ batchç»´åº¦
                        
                        # è¿›è¡Œæƒ…ç»ªé¢„æµ‹
                        emotion, confidence, probabilities = self.detector.predict_emotion(
                            visual_input, audio_input
                        )
                        
                        # è®°å½•ç»“æœ
                        result = {
                            'timestamp': current_time,
                            'emotion': emotion,
                            'confidence': confidence,
                            'probabilities': probabilities.tolist(),
                            'face_bbox': [x, y, w, h]
                        }
                        
                        self.emotion_history.append(result)
                        if self.save_results:
                            self.results_log.append(result)
                        
                        last_prediction_time = current_time
                    
                    # ä½¿ç”¨æœ€æ–°çš„é¢„æµ‹ç»“æœè¿›è¡Œæ˜¾ç¤º
                    if len(self.emotion_history) > 0:
                        latest_result = self.emotion_history[-1]
                        emotion = latest_result['emotion']
                        confidence = latest_result['confidence']
                        probabilities = np.array(latest_result['probabilities'])
                    
                    # åœ¨ç”»é¢ä¸Šç»˜åˆ¶ç»“æœ
                    color = self.detector.emotion_colors.get(emotion, (128, 128, 128))
                    
                    # ç»˜åˆ¶äººè„¸æ¡†
                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                    
                    # ç»˜åˆ¶æƒ…ç»ªæ ‡ç­¾
                    label = f"{emotion}: {confidence:.2f}"
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                    cv2.rectangle(frame, (x, y-label_size[1]-10), 
                                (x+label_size[0], y), color, -1)
                    cv2.putText(frame, label, (x, y-5), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    
                    # ç»˜åˆ¶æƒ…ç»ªæ¦‚ç‡æ¡
                    bar_width = 200
                    bar_height = 15
                    start_y = 30
                    
                    for i, (emotion_name, prob) in enumerate(zip(self.detector.emotion_labels, probabilities)):
                        y_pos = start_y + i * (bar_height + 5)
                        bar_length = int(bar_width * prob)
                        
                        # èƒŒæ™¯æ¡
                        cv2.rectangle(frame, (10, y_pos), (10 + bar_width, y_pos + bar_height), 
                                    (50, 50, 50), -1)
                        
                        # æ¦‚ç‡æ¡
                        emotion_color = self.detector.emotion_colors.get(emotion_name, (128, 128, 128))
                        cv2.rectangle(frame, (10, y_pos), (10 + bar_length, y_pos + bar_height), 
                                    emotion_color, -1)
                        
                        # æ ‡ç­¾
                        cv2.putText(frame, f"{emotion_name}: {prob:.2f}", 
                                  (10 + bar_width + 10, y_pos + bar_height - 3),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                
                # æ˜¾ç¤ºFPSå’Œç»Ÿè®¡ä¿¡æ¯
                fps = self.frame_count / current_time if current_time > 0 else 0
                cv2.putText(frame, f"FPS: {fps:.1f}", (frame.shape[1] - 100, 30),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                if len(self.emotion_history) > 0:
                    recent_emotion = self.emotion_history[-1]['emotion']
                    cv2.putText(frame, f"Current: {recent_emotion}", (frame.shape[1] - 150, 60),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('å®æ—¶æƒ…ç»ªç›‘æµ‹', frame)
                
                # æ›´æ–°å¯è§†åŒ–
                if self.frame_count % 60 == 0:  # æ¯60å¸§æ›´æ–°ä¸€æ¬¡ç»Ÿè®¡å›¾
                    self.update_visualization()
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    self.save_current_results()
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
        finally:
            # æ¸…ç†èµ„æº
            cap.release()
            cv2.destroyAllWindows()
            self.audio_capture.stop_recording()
            
            if self.show_stats:
                plt.ioff()
                plt.close()
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            if self.save_results and self.results_log:
                self.save_final_results()
                
            print("âœ… ç›‘æµ‹ç»“æŸ")
            
    def save_current_results(self):
        """ä¿å­˜å½“å‰ç»“æœ"""
        if not self.emotion_history:
            print("âš ï¸  æ²¡æœ‰å¯ä¿å­˜çš„ç»“æœ")
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"emotion_snapshot_{timestamp}.json"
        
        snapshot_data = {
            'timestamp': timestamp,
            'total_detections': len(self.emotion_history),
            'recent_emotions': list(self.emotion_history),
            'emotion_distribution': dict(Counter([r['emotion'] for r in self.emotion_history]))
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“¸ å½“å‰ç»“æœå·²ä¿å­˜: {filename}")
        
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"emotion_session_{timestamp}.json"
        
        session_data = {
            'session_info': {
                'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
                'end_time': datetime.now().isoformat(),
                'duration_seconds': time.time() - self.start_time,
                'total_frames': self.frame_count,
                'total_detections': len(self.results_log)
            },
            'emotion_statistics': {
                'distribution': dict(Counter([r['emotion'] for r in self.results_log])),
                'average_confidence': np.mean([r['confidence'] for r in self.results_log]),
                'dominant_emotion': Counter([r['emotion'] for r in self.results_log]).most_common(1)[0][0]
            },
            'detailed_results': self.results_log
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ å®Œæ•´ä¼šè¯ç»“æœå·²ä¿å­˜: {filename}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(session_data)
        
    def generate_report(self, session_data):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        print("\nğŸ“Š ä¼šè¯ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"   æŒç»­æ—¶é—´: {session_data['session_info']['duration_seconds']:.1f} ç§’")
        print(f"   æ€»å¸§æ•°: {session_data['session_info']['total_frames']}")
        print(f"   æ£€æµ‹æ¬¡æ•°: {session_data['session_info']['total_detections']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {session_data['emotion_statistics']['average_confidence']:.3f}")
        print(f"   ä¸»å¯¼æƒ…ç»ª: {session_data['emotion_statistics']['dominant_emotion']}")
        
        print("\nğŸ­ æƒ…ç»ªåˆ†å¸ƒ:")
        for emotion, count in session_data['emotion_statistics']['distribution'].items():
            percentage = count / session_data['session_info']['total_detections'] * 100
            print(f"   {emotion:>10}: {count:>3} æ¬¡ ({percentage:>5.1f}%)")


def main():
    parser = argparse.ArgumentParser(description='å®æ—¶æ‘„åƒå¤´æƒ…ç»ªç›‘æµ‹')
    parser.add_argument('--model_path', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--camera_id', type=int, default=0,
                       help='æ‘„åƒå¤´ID (é»˜è®¤: 0)')
    parser.add_argument('--save_results', action='store_true',
                       help='ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('--no_stats', action='store_true',
                       help='ä¸æ˜¾ç¤ºç»Ÿè®¡å›¾è¡¨')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not Path(args.model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
        
    # æ£€æŸ¥ä¾èµ–
    try:
        import cv2
        import pyaudio
        import librosa
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install opencv-python pyaudio librosa")
        return
    
    try:
        # åˆ›å»ºç›‘æµ‹å™¨
        monitor = RealtimeEmotionMonitor(
            model_path=args.model_path,
            save_results=args.save_results,
            show_stats=not args.no_stats,
            camera_id=args.camera_id
        )
        
        # å¼€å§‹ç›‘æµ‹
        monitor.run()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()