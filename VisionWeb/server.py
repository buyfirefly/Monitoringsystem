from flask import Flask, request, jsonify, render_template
from flask_socketio import SocketIO, emit
from flask_cors import CORS
import os
import threading
import time
import shutil
import sys
import cv2
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿè·¯å¾„
sys.path.append('../multimodal-emotion-recognition-system-master')
sys.path.append('../multimodal-emotion-recognition-system-master/emonet')
sys.path.append('../multimodal-emotion-recognition-system-master/multimodal_emotion')

try:
    from multimodal_emotion.fusion_model import MultimodalEmotionRecognizer, MultiModalEmotionModel
    from multimodal_emotion.realtime_inference import RealTimeEmotionRecognizer
    import torchvision.transforms as transforms
    from PIL import Image
except ImportError as e:
    print(f"Warning: Could not import multimodal emotion recognition modules: {e}")
    print("Please ensure the multimodal emotion recognition system is properly installed.")

app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!'
CORS(app)  # æ·»åŠ CORSæ”¯æŒ
socketio = SocketIO(app, cors_allowed_origins="*")

# è®¾ç½®ä¸Šä¼ æ–‡ä»¶å¤¹å’Œæœ€å¤§æ–‡ä»¶å¤§å°
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MB

# åˆ›å»ºä¸Šä¼ æ–‡ä»¶å¤¹
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# æƒ…ç»ªè¯†åˆ«æ¨¡å‹é…ç½®
EMOTION_MODEL = None
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
IMAGE_SIZE = 224
EMOTION_CLASSES = ['anger', 'disgust', 'sadness', 'joy', 'neutral', 'surprise', 'fear']
EMOTION_EMOJIS = {
    'anger': 'ğŸ˜ ', 'disgust': 'ğŸ¤¢', 'sadness': 'ğŸ˜¢',
    'joy': 'ğŸ˜„', 'neutral': 'ğŸ˜', 'surprise': 'ğŸ˜®', 'fear': 'ğŸ˜¨'
}

# å›¾åƒé¢„å¤„ç†
image_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# åˆå§‹åŒ–æƒ…ç»ªè¯†åˆ«æ¨¡å‹
def initialize_emotion_model():
    global EMOTION_MODEL
    try:
        # åŠ è½½å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«æ¨¡å‹
        model_path = '../best_model.pth'
        print(f"æ­£åœ¨æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {model_path}")
        print(f"æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {os.path.exists(model_path)}")
        
        if os.path.exists(model_path):
            print("å¼€å§‹åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹...")
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
            print(f"æ£€æŸ¥ç‚¹é”®: {list(checkpoint.keys())}")
            
            print("åˆ›å»ºMultimodalEmotionRecognizerå®ä¾‹...")
            # ä½¿ç”¨MultimodalEmotionRecognizerï¼ˆä¸checkpointç»“æ„åŒ¹é…ï¼‰
            # æ ¹æ®æ£€æŸ¥ç‚¹ä¸­çš„transformer_encoderæƒé‡ï¼Œä½¿ç”¨transformerèåˆç±»å‹
            EMOTION_MODEL = MultimodalEmotionRecognizer(
                num_emotions=7,
                d_model=256,
                n_heads=8,
                fusion_type="transformer"
            )
            
            print("åŠ è½½æ¨¡å‹æƒé‡...")
            # åŠ è½½æƒé‡
            EMOTION_MODEL.load_state_dict(checkpoint['model_state_dict'])
            EMOTION_MODEL.to(DEVICE)
            EMOTION_MODEL.eval()
            
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            push_message('å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸï¼', '#006400')
            return True
        else:
            print(f"é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            push_message('æœªæ‰¾åˆ°best_model.pthæ–‡ä»¶', '#FF0000')
            return False
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¼‚å¸¸: {str(e)}")
        print(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        push_message(f'æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}', '#FF0000')
        return False

# å¤„ç†è§†é¢‘æƒ…ç»ªè¯†åˆ«
def process_video_emotion(video_path):
    try:
        if EMOTION_MODEL is None:
            return {'error': 'æƒ…ç»ªè¯†åˆ«æ¨¡å‹æœªåŠ è½½'}
        
        # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {'error': 'æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶'}
        
        emotion_results = []
        frame_count = 0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # äººè„¸æ£€æµ‹å™¨
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
        # åˆ†ææ¯ä¸€å¸§
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            try:
                # è½¬æ¢ä¸ºRGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # æ£€æµ‹äººè„¸ - å¤šå°ºåº¦æ£€æµ‹æé«˜æˆåŠŸç‡
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # å›¾åƒé¢„å¤„ç† - å¢å¼ºå¯¹æ¯”åº¦
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                gray = clahe.apply(gray)
                
                # å¤šå°ºåº¦äººè„¸æ£€æµ‹ - å°è¯•ä¸åŒå‚æ•°ç»„åˆ
                faces = []
                
                # ç¬¬ä¸€æ¬¡æ£€æµ‹ï¼šé«˜ç²¾åº¦æ£€æµ‹
                faces1 = face_cascade.detectMultiScale(
                    gray, 
                    scaleFactor=1.03,  # æ›´å°çš„ç¼©æ”¾æ­¥é•¿
                    minNeighbors=2,    # é™ä½é‚»å±…è¦æ±‚
                    minSize=(20, 20),  # æ›´å°çš„æœ€å°å°ºå¯¸
                    maxSize=(800, 800), # æ›´å¤§çš„æœ€å¤§å°ºå¯¸
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(faces1) > 0:
                    faces = faces1
                else:
                    # ç¬¬äºŒæ¬¡æ£€æµ‹ï¼šæ›´å®½æ¾çš„å‚æ•°
                    faces2 = face_cascade.detectMultiScale(
                        gray, 
                        scaleFactor=1.1,
                        minNeighbors=1,
                        minSize=(15, 15),
                        maxSize=(1000, 1000),
                        flags=cv2.CASCADE_DO_CANNY_PRUNING
                    )
                    
                    if len(faces2) > 0:
                        faces = faces2
                    else:
                        # ç¬¬ä¸‰æ¬¡æ£€æµ‹ï¼šæœ€å®½æ¾çš„å‚æ•°
                        faces3 = face_cascade.detectMultiScale(
                            gray, 
                            scaleFactor=1.2,
                            minNeighbors=1,
                            minSize=(10, 10)
                        )
                        faces = faces3
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                print(f"ç¬¬ {frame_count} å¸§æ£€æµ‹ç»“æœ: å‘ç° {len(faces)} ä¸ªäººè„¸")
                
                if len(faces) > 0:
                    # å–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
                    x, y, w, h = faces[0]
                    print(f"äººè„¸ä½ç½®: x={x}, y={y}, w={w}, h={h}")
                    
                    # è£å‰ªäººè„¸åŒºåŸŸ
                    face_crop = frame_rgb[y:y+h, x:x+w]
                    
                    if face_crop.size > 0:
                        # æ·»åŠ è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹äººè„¸å›¾åƒçš„ç»Ÿè®¡ä¿¡æ¯
                        print(f"åŸå§‹äººè„¸å›¾åƒç»Ÿè®¡: mean={face_crop.mean():.3f}, std={face_crop.std():.3f}, min={face_crop.min()}, max={face_crop.max()}")
                        
                        # é¢„å¤„ç†å›¾åƒ
                        face_pil = Image.fromarray(face_crop)
                        face_tensor = image_transform(face_pil).unsqueeze(0).to(DEVICE)
                        
                        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥é¢„å¤„ç†åçš„tensorç»Ÿè®¡
                        print(f"face_tensor shape: {face_tensor.shape}")
                        print(f"é¢„å¤„ç†åtensorç»Ÿè®¡: mean={face_tensor.mean():.6f}, std={face_tensor.std():.6f}, min={face_tensor.min():.6f}, max={face_tensor.max():.6f}")
                        
                        # è¿è¡Œæƒ…ç»ªè¯†åˆ«ï¼ˆå¤šæ¨¡æ€èåˆä¼˜åŒ–ç‰ˆ - å¢å¼ºæƒ…ç»ªæ•æ„Ÿåº¦ï¼‰
                        if EMOTION_MODEL is not None:
                            try:
                                with torch.no_grad():
                                    # é¦–å…ˆæå–è§†è§‰ç‰¹å¾ç”¨äºç”Ÿæˆæ™ºèƒ½éŸ³é¢‘è¾“å…¥
                                    visual_features, visual_seq = EMOTION_MODEL.visual_encoder(face_tensor)
                                    
                                    # åŸºäºè§†è§‰ç‰¹å¾ç”Ÿæˆæ™ºèƒ½éŸ³é¢‘è¾“å…¥
                                    # åˆ†æé¢éƒ¨è¡¨æƒ…å¼ºåº¦æ¥è°ƒæ•´éŸ³é¢‘ç‰¹å¾
                                    visual_intensity = torch.abs(visual_features).mean().item()
                                    
                                    # åˆ›å»ºåŸºäºè§†è§‰çº¿ç´¢çš„éŸ³é¢‘ç‰¹å¾
                                    # ä½¿ç”¨è§†è§‰ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯æ¥æŒ‡å¯¼éŸ³é¢‘ç”Ÿæˆ
                                    base_freq = 0.1 + visual_intensity * 0.05  # åŸºç¡€é¢‘ç‡
                                    amplitude = 0.05 + visual_intensity * 0.03  # æŒ¯å¹…
                                    
                                    # ç”Ÿæˆæ›´çœŸå®çš„æ¢…å°”é¢‘è°±å›¾
                                    # æ¨¡æ‹Ÿä¸åŒæƒ…ç»ªçš„éŸ³é¢‘ç‰¹å¾æ¨¡å¼
                                    time_steps = 100
                                    mel_channels = 64
                                    
                                    # åˆ›å»ºæ—¶é—´åºåˆ—ï¼Œæ·»åŠ å¸§ç´¢å¼•ä½œä¸ºæ—¶é—´åç§»
                                    frame_offset = (frame_count % 100) * 0.01  # åŸºäºå¸§æ•°çš„æ—¶é—´åç§»
                                    t = torch.linspace(frame_offset, 1 + frame_offset, time_steps)
                                    
                                    # ç”Ÿæˆå¤šé¢‘ç‡æˆåˆ†çš„éŸ³é¢‘ç‰¹å¾
                                    mel_spec = torch.zeros(1, mel_channels, time_steps).to(DEVICE)
                                    
                                    # æ·»åŠ åŸºäºå¸§æ•°çš„éšæœºç§å­å˜åŒ–
                                    torch.manual_seed(frame_count + int(visual_intensity * 1000))
                                    
                                    for i in range(mel_channels):
                                        # ä¸ºæ¯ä¸ªæ¢…å°”é¢‘é“ç”Ÿæˆä¸åŒçš„é¢‘ç‡æˆåˆ†
                                        freq_factor = (i + 1) / mel_channels
                                        
                                        # æ·»åŠ å¸§ç›¸å…³çš„é¢‘ç‡è°ƒåˆ¶
                                        frame_modulation = 1.0 + 0.1 * torch.sin(torch.tensor(frame_count * 0.1))
                                        
                                        # åŸºäºè§†è§‰ç‰¹å¾è°ƒæ•´é¢‘ç‡æ¨¡å¼
                                        if visual_intensity > 0.3:  # é«˜å¼ºåº¦è¡¨æƒ…
                                            # ç”Ÿæˆæ›´æ´»è·ƒçš„éŸ³é¢‘æ¨¡å¼ï¼ˆå¦‚ç¬‘å£°ã€æƒŠè®¶ï¼‰
                                            signal = amplitude * torch.sin(2 * 3.14159 * base_freq * freq_factor * 3 * t * frame_modulation)
                                            signal += amplitude * 0.5 * torch.sin(2 * 3.14159 * base_freq * freq_factor * 7 * t * frame_modulation)
                                        else:  # ä½å¼ºåº¦è¡¨æƒ…
                                            # ç”Ÿæˆæ›´å¹³ç¼“çš„éŸ³é¢‘æ¨¡å¼ï¼ˆå¦‚ä¸­æ€§ã€æ‚²ä¼¤ï¼‰
                                            signal = amplitude * 0.7 * torch.sin(2 * 3.14159 * base_freq * freq_factor * t * frame_modulation)
                                        
                                        # æ·»åŠ æ›´å¤šéšæœºå™ªå£°å’Œå¸§å˜åŒ–
                                        noise = torch.randn_like(signal) * (0.01 + 0.005 * (frame_count % 10) / 10)
                                        frame_variation = 0.02 * torch.sin(torch.tensor(frame_count * 0.05 + i * 0.1))
                                        mel_spec[0, i, :] = signal + noise + frame_variation
                                    
                                    # åº”ç”¨æ¢…å°”é¢‘è°±çš„å…¸å‹ç‰¹å¾ï¼ˆä½é¢‘æ›´å¼ºï¼‰
                                    freq_weights = torch.exp(-torch.arange(mel_channels, dtype=torch.float32) * 0.05).to(DEVICE)
                                    mel_spec = mel_spec * freq_weights.view(1, -1, 1)
                                    
                                    print(f"è§†è§‰ç‰¹å¾ç»Ÿè®¡: mean={visual_features.mean():.6f}, std={visual_features.std():.6f}")
                                    print(f"è§†è§‰å¼ºåº¦: {visual_intensity:.6f}")
                                    print(f"æ™ºèƒ½éŸ³é¢‘ç‰¹å¾ç»Ÿè®¡: mean={mel_spec.mean():.6f}, std={mel_spec.std():.6f}")
                                    print(f"éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {mel_spec.shape}")
                                    
                                    # ä½¿ç”¨å®Œæ•´çš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œæ¨ç†
                                    outputs = EMOTION_MODEL(face_tensor, mel_spec)
                                    
                                    # æƒ…ç»ªæ•æ„Ÿåº¦å¹³è¡¡å¤„ç†
                                    # 1. è°ƒæ•´softmaxæ¸©åº¦å‚æ•°ï¼Œä¿æŒé€‚åº¦æ•æ„Ÿåº¦
                                    temperature = 0.7  # é€‚ä¸­çš„æ¸©åº¦å‚æ•°
                                    
                                    # 2. å¹³è¡¡çš„æƒé‡è°ƒæ•´ç­–ç•¥
                                    emotion_weights = torch.ones_like(outputs).to(DEVICE)
                                    neutral_idx = EMOTION_CLASSES.index('neutral')
                                    joy_idx = EMOTION_CLASSES.index('joy')
                                    anger_idx = EMOTION_CLASSES.index('anger')
                                    surprise_idx = EMOTION_CLASSES.index('surprise')
                                    sadness_idx = EMOTION_CLASSES.index('sadness')
                                    disgust_idx = EMOTION_CLASSES.index('disgust')
                                    fear_idx = EMOTION_CLASSES.index('fear')
                                    
                                    # å¹³è¡¡çš„æƒé‡åˆ†é…
                                    emotion_weights[0, neutral_idx] = 0.8   # é€‚åº¦é™ä½ä¸­æ€§
                                    emotion_weights[0, joy_idx] = 1.2       # é€‚åº¦å¢å¼ºå¿«ä¹
                                    emotion_weights[0, sadness_idx] = 1.2   # é€‚åº¦å¢å¼ºæ‚²ä¼¤
                                    emotion_weights[0, anger_idx] = 1.1     # è½»å¾®å¢å¼ºæ„¤æ€’
                                    emotion_weights[0, surprise_idx] = 1.1  # è½»å¾®å¢å¼ºæƒŠè®¶
                                    emotion_weights[0, disgust_idx] = 1.0   # ä¿æŒåŒæ¶
                                    emotion_weights[0, fear_idx] = 1.0      # ä¿æŒææƒ§
                                    
                                    # åº”ç”¨æƒé‡è°ƒæ•´
                                    adjusted_outputs = outputs * emotion_weights
                                    
                                    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
                                    scaled_outputs = adjusted_outputs / temperature
                                    
                                    # è®¡ç®—è°ƒæ•´åçš„æ¦‚ç‡
                                    emotion_probs = torch.nn.functional.softmax(scaled_outputs, dim=1)
                                    predicted_emotion_idx = torch.argmax(emotion_probs).item()
                                    confidence = emotion_probs[0][predicted_emotion_idx].item()
                                    
                                    # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                                    print(f"åŸå§‹è¾“å‡º: {outputs[0].detach().cpu().numpy()}")
                                    print(f"æƒé‡è°ƒæ•´å: {adjusted_outputs[0].detach().cpu().numpy()}")
                                    print(f"æ¸©åº¦ç¼©æ”¾å: {scaled_outputs[0].detach().cpu().numpy()}")
                                    print(f"æœ€ç»ˆæ¦‚ç‡åˆ†å¸ƒ: {emotion_probs[0].detach().cpu().numpy()}")
                                    print(f"æƒ…ç»ªæƒé‡: {emotion_weights[0].detach().cpu().numpy()}")
                                    print(f"æ¸©åº¦å‚æ•°: {temperature}")
                                    print(f"å„æƒ…ç»ªæ¦‚ç‡:")
                                    for i, emotion in enumerate(EMOTION_CLASSES):
                                        prob = emotion_probs[0][i].item()
                                        weight = emotion_weights[0][i].item()
                                        print(f"  {emotion}: {prob:.4f} (æƒé‡: {weight:.1f})")
                                    print(f"é¢„æµ‹æƒ…æ„Ÿ: {EMOTION_CLASSES[predicted_emotion_idx]}, ç½®ä¿¡åº¦: {confidence:.3f}")
                                    
                                    emotion_results.append({
                                        'frame': frame_count,
                                        'emotion': EMOTION_CLASSES[predicted_emotion_idx],
                                        'confidence': float(confidence)
                                    })
                            except Exception as e:
                                print(f"æ¨¡å‹é¢„æµ‹é”™è¯¯: {e}")
                                print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
                                import traceback
                                traceback.print_exc()
                                emotion_results.append({
                                    'frame': frame_count,
                                    'emotion': 'prediction_error',
                                    'confidence': 0.0
                                })
                        else:
                            # æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨éšæœºæƒ…æ„Ÿä½œä¸ºå ä½ç¬¦
                            import random
                            predicted_emotion = random.choice(EMOTION_CLASSES)
                            confidence = random.uniform(0.6, 0.9)
                            
                            print(f"æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨éšæœºæƒ…æ„Ÿ: {predicted_emotion}, ç½®ä¿¡åº¦: {confidence:.3f}")
                            
                            emotion_results.append({
                                'frame': frame_count,
                                'emotion': predicted_emotion,
                                'confidence': confidence
                            })
                else:
                    print(f"ç¬¬ {frame_count} å¸§æœªæ£€æµ‹åˆ°äººè„¸ - å¸§å°ºå¯¸: {frame.shape}")
                    emotion_results.append({
                        'frame': frame_count,
                        'emotion': 'no_face_detected',
                        'confidence': 0.0
                    })
                     
            except Exception as e:
                emotion_results.append({
                    'frame': frame_count,
                    'error': str(e)
                })
            
            frame_count += 1
        
        cap.release()
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        valid_results = [r for r in emotion_results if 'emotion' in r and r['emotion'] not in ['no_face_detected', 'prediction_error']]
        
        if valid_results:
            # è®¡ç®—å¹³å‡æƒ…ç»ª
            emotion_counts = {}
            total_confidence = 0
            
            for result in valid_results:
                emotion = result['emotion']
                confidence = result['confidence']
                
                if emotion not in emotion_counts:
                    emotion_counts[emotion] = {'count': 0, 'total_confidence': 0}
                
                emotion_counts[emotion]['count'] += 1
                emotion_counts[emotion]['total_confidence'] += confidence
                total_confidence += confidence
            
            # æ‰¾å‡ºä¸»è¦æƒ…ç»ª
            dominant_emotion = max(emotion_counts.keys(), key=lambda x: emotion_counts[x]['count'])
            avg_confidence = total_confidence / len(valid_results)
            
            summary = {
                'total_frames': total_frames,
                'frames_with_faces': len(valid_results),
                'dominant_emotion': dominant_emotion,
                'average_confidence': avg_confidence,
                'emotion_distribution': {emotion: counts['count'] for emotion, counts in emotion_counts.items()},
                'detailed_results': emotion_results[:10]  # åªè¿”å›å‰10å¸§çš„è¯¦ç»†ç»“æœ
            }
        else:
            summary = {
                'total_frames': total_frames,
                'frames_with_faces': 0,
                'error': 'è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°äººè„¸'
            }
        
        return summary
        
    except Exception as e:
        return {'error': f'è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}'}


# ç”¨äºå‘å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯
def push_message(message, color):
    socketio.emit('server_message', {'message': message, 'color': color})

@app.route('/')
def index():
    return render_template('index.html')


@app.route('/upload', methods=['POST'])
def upload_file():
    if 'video' not in request.files:
        push_message('æ²¡æœ‰è§†é¢‘æ–‡ä»¶', '#FF0000')  # çº¢è‰²ï¼Œè¡¨ç¤ºé”™è¯¯
        return jsonify({'message': 'æ²¡æœ‰è§†é¢‘æ–‡ä»¶'}), 400
    file = request.files['video']
    if file.filename == '':
        push_message('æ²¡æœ‰é€‰æ‹©æ–‡ä»¶', '#FF0000')  # çº¢è‰²ï¼Œè¡¨ç¤ºé”™è¯¯
        return jsonify({'message': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    if file:
        filename = file.filename
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        push_message(f'è§†é¢‘ {filename} ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹æƒ…ç»ªåˆ†æ...', '#006400')  # ç»¿è‰²ï¼Œè¡¨ç¤ºæˆåŠŸ
        
        # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†æƒ…ç»ªè¯†åˆ«
        def process_emotion_async():
            try:
                # è¿›è¡Œæƒ…ç»ªè¯†åˆ«
                emotion_result = process_video_emotion(filepath)
                
                if 'error' in emotion_result:
                    push_message(f'æƒ…ç»ªåˆ†æå¤±è´¥: {emotion_result["error"]}', '#FF0000')
                else:
                    # æ ¼å¼åŒ–ç»“æœæ¶ˆæ¯
                    if 'dominant_emotion' in emotion_result:
                        dominant_emotion = emotion_result['dominant_emotion']
                        confidence = emotion_result['average_confidence']
                        frames_with_faces = emotion_result['frames_with_faces']
                        total_frames = emotion_result['total_frames']
                        
                        emoji = EMOTION_EMOJIS.get(dominant_emotion, 'ğŸ¤”')
                        
                        result_message = f"""ğŸ¬ è§†é¢‘æƒ…ç»ªåˆ†æå®Œæˆï¼
{emoji} ä¸»è¦æƒ…ç»ª: {dominant_emotion}
ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {confidence:.2f}
ğŸ‘¤ æ£€æµ‹åˆ°äººè„¸å¸§æ•°: {frames_with_faces}/{total_frames}

ğŸ“ˆ æƒ…ç»ªåˆ†å¸ƒ:"""
                        
                        for emotion, count in emotion_result['emotion_distribution'].items():
                            percentage = (count / frames_with_faces) * 100 if frames_with_faces > 0 else 0
                            emoji = EMOTION_EMOJIS.get(emotion, 'ğŸ¤”')
                            result_message += f"\n{emoji} {emotion}: {count}å¸§ ({percentage:.1f}%)"
                        
                        push_message(result_message, '#0066CC')  # è“è‰²ï¼Œè¡¨ç¤ºåˆ†æç»“æœ
                    else:
                        push_message(f'æƒ…ç»ªåˆ†æå®Œæˆï¼Œä½†{emotion_result.get("error", "æœªçŸ¥é”™è¯¯")}', '#FF8800')
                        
            except Exception as e:
                push_message(f'æƒ…ç»ªåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}', '#FF0000')
        
        # å¯åŠ¨åå°å¤„ç†çº¿ç¨‹
        threading.Thread(target=process_emotion_async, daemon=True).start()
        
        return jsonify({
            'message': f'è§†é¢‘ {filename} ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨è¿›è¡Œæƒ…ç»ªåˆ†æ...',
            'path': filepath,
            'filename': filename,
            'size': file.content_length,
            'type': file.mimetype,
            'status': 'processing'
        })

    push_message('ä¸Šä¼ å¤±è´¥', '#FF0000')  # çº¢è‰²ï¼Œè¡¨ç¤ºé”™è¯¯
    return jsonify({'message': 'ä¸Šä¼ å¤±è´¥'}), 500



def handle_user_input():
    while True:
        user_input = input("è¯·è¾“å…¥æµ‹è¯•ä¿¡æ¯ (æˆ–è¾“å…¥ 'exit' é€€å‡º): ")
        if user_input.lower() == 'exit':
            break
        push_message(user_input, '#006400')  # ç»¿è‰²æ–‡æœ¬


if __name__ == '__main__':
    print("æ­£åœ¨å¯åŠ¨æƒ…ç»ªè¯†åˆ«è§†é¢‘åˆ†ææœåŠ¡å™¨...")
    
    # åˆå§‹åŒ–æƒ…ç»ªè¯†åˆ«æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹...")
    model_loaded = initialize_emotion_model()
    
    if model_loaded:
        print("âœ… æƒ…ç»ªè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    else:
        print("âš ï¸  æƒ…ç»ªè¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä»…æä¾›è§†é¢‘ä¸Šä¼ åŠŸèƒ½")
    
    print("ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    print("ğŸ¥ ç°åœ¨å¯ä»¥ä¸Šä¼ è§†é¢‘è¿›è¡Œæƒ…ç»ªåˆ†æäº†ï¼")
    
    socketio.run(app, debug=True, allow_unsafe_werkzeug=True, host='0.0.0.0', port=5000)